Maria Bartnik - VR lab 1

The objective of this project was to develop an interactive virtual reality (VR) simulation that would cover the United Nations Sustainable Development Goals (SDGs), particularly goals 11 (Sustainable Cities and Communities), 12 (Responsible Consumption and Production), and 13 (Climate Action). These goals emphasize the need for sustainable urban environments, responsible management of resources, and awareness of environmental impact. By creating a VR-based educational tool, we aimed to provide users with an engaging and immersive method to understand recycling practices in a realistic context.

At the beginning of our development, we conducted a planning session to define the scope and objectives. We wanted to create a VR environment where users could practice sorting waste into appropriate bins and therefore learn about proper trash disposal.
The first step involved creating a prototype trash bin. We searched the Unity Asset Store and selected a generic 3D trash bin model. Once imported, the asset was placed in a basic scene containing only a simple plane to act as a temporary floor.

To enable the trash bin to recognize incoming waste, we inserted a trigger collider inside the bin. We then attached a C# script named TouchTrash.cs to the collider. The script was designed to monitor collisions: whenever another object entered the trigger area, it would check the object’s tag to determine if it matched the tag of waste the bin was intended to collect. If the tags matched, the script would immediately remove the object from the scene, effectively simulating proper disposal.

To verify that the system worked as intended, we first used a simple sphere as a temporary waste object. This sphere was assigned the same tag as the bin, using “glass” as an example, and was manually moved into the bin during simulation. When the object disappeared as expected, it confirmed that the collider and script were functioning correctly.

Further on, we added World Space Canvas in front of each bin. A World Space Canvas is a type of Unity UI element that exists within the 3D environment rather than being overlaid on the screen. It allowed us to tag each bin with the type of trash we wanted to be responsible for.

After successfully implementing the first bin, we saved it as a prefab inside of our VR project. Then, we created new bins based on the original prefab as prefab variants while modifying properties such as color, name, and tag. 
We color-coded the bins according to Danish recycling standards. For example, blue bins were designated for paper, green for glass, yellow for plastics, brown for organic waste, and black for general waste. In total, we created nine bins representing different types of recyclable materials. Afterwards, we made sure all of the trash bin variants had their appropriate tags, for the touchTrash script to be reused across all bins without any modifications.

With the bins functional, we turned our attention to the VR environment. We selected a restaurant scene from the Unity Asset Store as the setting for the simulation. Restaurants provide a natural variety of waste types, including food, packaging, and recyclables, making them ideal for a recycling education simulation.

After importing the restaurant scene into an empty Unity project, we prepared it for VR interaction by adding the XR Interaction Manager and XR Origin prefabs from Unity’s XR Interaction Toolkit. The XR Interaction Manager coordinates all interactions between VR controllers and interactable objects, ensuring that events such as grabbing, throwing, or triggering actions are correctly handled. The XR Origin represents the player’s position and orientation in the VR world, mapping their physical movements and headset orientation to the virtual environment. We then added a single slice of pizza as a placeholder interactable object representing food waste and added the prefabs of trash bins in a row next to each other, under an empty wall.
When we first ran the scene in the Unity simulator, we were met with a problem: the XR Origin fell straight through the floor. Because the restaurant environment, as downloaded from the Asset Store, did not include properly configured colliders for the floor mesh. To resolve this, we added a simple plane beneath the environment with a plane collider, just below the asset’s flooring, which prevented the player from falling through the scene.

We also observed that other objects in the scene required colliders. For example, the pizza slice fell through the floor, and the XR Origin could walk through tables and chairs. To fix this, we applied colliders to complex objects like furniture and walls, for which the mesh collider worked just fine, but also box colliders for objects like our pizza slice, for whose mesh collider did not register. Finally, colliders were added to all walls in the restaurant, preventing the player from walking outside the environment and maintaining immersion. These adjustments highlighted the importance of thorough collider management in VR projects, especially when using pre-made assets.
Even though we didn’t yet get to test the project in VR since we encountered some technical issues, the first session was very productive. We successfully came up with the concept of the project, designed a visually appealing environment, imported and customized assets, and began implementing the first mechanics. The process also gave us a good introduction to how VR projects are structured differently from AR ones.

