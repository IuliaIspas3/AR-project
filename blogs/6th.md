Dominika Janczyszyn - VR Lab 2

During the second lab session of our VR development project, our main objective was to expand the functionality of our virtual environment by introducing collision detection, basic locomotion, and headset integration. Building on the foundational scene created during the first session, this lab focused on implementing interaction mechanics that would make the environment feel physically consistent and navigable from a first-person VR perspective. We also began preparing the project for deployment to the Meta VR headset and tested the application both within Unity and directly on the device.
We started the session by adding collision systems to the objects and surfaces present in our VR scene. To achieve reliable physical interactions, we assigned appropriate collider components—such as Box Colliders, Sphere Colliders, and Mesh Colliders—to various environmental elements. Flat surfaces like floors and walls were given box colliders, while more irregularly shaped objects received mesh colliders for improved accuracy. We ensured that each collider matched the geometry of the corresponding object to avoid issues such as clipping, falling through surfaces, or inaccurate collision responses. This step provided the foundational physics structure necessary for realistic navigation and object interaction within the VR world.
Once collision detection was in place, we moved on to implementing movement and head-tracking functionality. Using Unity’s XR Interaction Toolkit and custom C# scripts, we added support for VR head movement by linking the in-game camera to the headset’s positional and rotational tracking. This allowed the player’s viewpoint to accurately follow real-world head motions. For locomotion, we created simple walking mechanics that allowed the user to move forward in the virtual environment. The movement script interpreted input from the VR controller or the headset’s direction, translating it into player motion. In addition, we ensured the Rigidbody components and character controller logic worked properly with the collider system so that movement felt natural, stable, and constrained by the environment.
A significant part of this session was also dedicated to preparing the project for deployment on the Meta headset. To facilitate this, we configured Unity’s build settings by switching the platform to Android, enabling relevant XR plugins, and adjusting rendering, performance, and input settings to match the requirements of the device. We installed the necessary Meta XR packages and verified that the project recognized the headset as the target platform. This setup process involved adjusting player settings such as permissions, ARM architecture, and VR initialization options to ensure compatibility.
After the build environment was correctly configured, we proceeded to test the application in two ways: first through Unity’s Play Mode using simulated VR inputs, and then directly on the Meta headset itself. Testing in Unity allowed us to quickly verify that the movement scripts and collision systems behaved as expected. Once the build was deployed to the Meta headset, we were able to run the VR app natively and observe how the interactions translated to an actual VR environment. The collision system functioned properly on the headset, and the head-tracking and walking mechanics felt smooth and responsive. This real-device testing provided valuable insights into user experience and highlighted the performance considerations unique to standalone VR hardware.
One of the challenges we encountered during the session was ensuring that colliders were properly aligned with the visual mesh of the environment. In several cases, collider boundaries needed manual adjustment to prevent the player from passing through geometry or getting stuck. We also faced minor difficulties with the movement system, such as fine-tuning speed values and ensuring that locomotion did not conflict with physics constraints. Additionally, the initial setup for Meta headset deployment required troubleshooting due to plugin configuration differences and build errors, but these were resolved through adjustments to project and XR settings.
By the end of the session, we successfully established a functional movement and collision system and verified that our VR application could run on the Meta headset. These improvements significantly enhanced the interactivity and realism of the environment, laying the groundwork for more advanced features such as object manipulation, environmental triggers, and gameplay mechanics in future sessions.
This lab helped us gain practical experience with Unity’s physics system, VR input tracking, and platform-specific deployment processes. Overall, the session was productive and marked an important step forward in transforming our VR scene from a static environment into an interactive and immersive experience.
